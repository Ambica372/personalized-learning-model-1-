# Intellia ğŸ§   
*Cognitive-Adaptive Learning System*

Intellia is a Python-based system that measures **how a student thinks** and adapts learning content based on that cognitive profile.

Instead of predicting grades or labeling intelligence, Intellia converts raw cognitive test scores into **interpretable cognitive ability percentiles** and uses them to guide how content should be explained.

---

## ğŸš€ What This Project Does

- Takes raw cognitive test scores as input
- Converts them into **6 cognitive ability scores**
- Uses explainable rules to decide *how* content should be taught
- Outputs adaptation instructions (ready for AI-generated content)

This repository contains the **complete backend logic** for cognitive measurement and adaptation.

---

## ğŸ§© Core Concept

**Pipeline:**

Raw Test Scores
â†“
Cognitive Ability Measurement (PCA)
â†“
Percentile Scoring (0â€“1)
â†“
Rule-Based Teaching Strategy
â†“
Adapted Learning Instructions


No black-box AI is used for scoring or decision-making.

---

## ğŸ§  Cognitive Abilities (Pillars)

Each student is measured on **six independent abilities**:

1. **Attention**
2. **Thinking Conversion**
3. **Information Processing Ability**
4. **Logical Reasoning**
5. **Representational Ability**
6. **Memory**

Each pillar outputs a value between `0.0` and `1.0`, representing relative performance within the trained reference distribution.

---

## ğŸ— Project Structure

intellia/
â”œâ”€â”€ train_models.py # Offline training script
â”œâ”€â”€ app.py # Runtime scoring & adaptation
â”œâ”€â”€ adaptation_rules.py # Teaching strategy logic
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ models/ # Saved model artifacts (.pkl)


---

## ğŸ”¹ Offline Training (`train_models.py`)

This script trains the cognitive measurement models.

### What it does
- Loads a cognitive test dataset (`cognitive_dataset.csv`)
- Normalizes all metrics so **higher = better**
- Trains a PCA model for each cognitive pillar
- Saves reusable model artifacts to `/models`

### Output files (per pillar)
- `<pillar>_scaler.pkl`
- `<pillar>_pca.pkl`
- `<pillar>_pc_reference.pkl`

These artifacts are required for runtime scoring.

### How to run
```bash
python train_models.py
ğŸ”¹ Runtime Scoring & Adaptation (app.py)

This script:

Loads pretrained model artifacts

Scores a studentâ€™s cognitive abilities

Converts scores into percentiles

Applies adaptation rules

Prints the cognitive profile and teaching strategy

How to run
python app.py

ğŸ”¹ Teaching Logic (adaptation_rules.py)

This file contains deterministic, explainable rules that define how learning content should change based on cognitive strengths and weaknesses.

Examples:

Low memory â†’ repeat key ideas

Low processing speed â†’ simpler language

High reasoning â†’ deeper explanations

No machine learning is used here.

ğŸ“¦ Requirements

Install dependencies using:

pip install -r requirements.txt


Dependencies:

Python 3.9+

numpy

pandas

scikit-learn

joblib

âŒ What This Project Is NOT

Not a grading system

Not an intelligence predictor

Not supervised learning

Not a black-box AI system

AI (e.g. LLMs) are intended only for content generation, not scoring.

ğŸ§  Design Principles

Explainability over prediction

Measurement over labeling

Rules over opaque models

Separation of concerns:

Measurement

Teaching strategy

Content generation

ğŸ“Œ Typical Workflow

Prepare cognitive_dataset.csv

Run train_models.py

Place generated .pkl files in /models

Run app.py to score students and generate adaptation logic

ğŸ§¾ One-Line Summary

Intellia measures how students think and adapts how learning content is explained â€” not what is taught.
